{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recommender Systems: implicit.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyML615zPGSd6aO0f1hak1bb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yexf308/AdvancedMachineLearning/blob/main/Recommender_Systems_implicit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kS_NIKe7EDi"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\def\\m#1{\\mathbf{#1}}$\n",
        "$\\def\\mb#1{\\mathbb{#1}}$\n",
        "$\\def\\c#1{\\mathcal{#1}}$\n",
        "\n",
        "# Implicit feedback\n",
        "We have assumed that the user gives explicit ratings for each item that they interact with.\n",
        "This is a very restrictive assumption. More generally, we would like to learn from the **implicit feedback** that users give just by interacting with a system.\n",
        "\n",
        "we can view the fact that they watched movie $i$ but did not watch movie $j$ as an\n",
        "implicit signal that they prefer $i$ to $j$. \n",
        "\n",
        "Even when we do have explicit data, it might still be a good idea to incorporate implicit data into the model. Consider, for example, listening to songs. When users listen to music on a streaming service, they might rarely ever rate a song that he/she like or dislike. But more often they skip a song, or listen only halfway through it if they dislike it. If the user really liked a song, they will often come back and listen to it. So, to infer a user's musical taste profile, their listens, repeat listens, skips and fraction of tracks listened to, etc. might be far more valuable signals than explicit ratings.\n",
        "\n",
        "<img src=\"https://github.com/yexf308/AdvancedMachineLearning/blob/main/image/implicit1.png?raw=true\" width=\"300\" />\n",
        "\n",
        "### Possible method\n",
        "- Treat all the missing entries as zeros and minimizing the loss on all the entries. \n",
        "\n",
        "- 95% elements are zero and it tend to fit zeroes instead of ones. \n",
        "\n",
        "<img src=\"https://github.com/yexf308/AdvancedMachineLearning/blob/main/image/implicit2.png?raw=true\" width=\"500\" />\n",
        "\n",
        "\n",
        "- Or one can use the exact the same method in the explicit feedback. We make the assumption that if a user has interacted at all with an item, if the user $u$ has a preference for item $i$, then $A_{ui}=1$. However, when $A_{ui}=0$, we assume that it should be associated with a lower confidence, as there are many reasons beyond disliking the item as to why the user has not interacted with it,  e.g. Unaware of its existence.\n",
        "\n",
        "## Method 1: Weighted matrix factorization \n",
        "- Give lower weights to 0 and higher weights to 1:\n",
        "\\begin{align}\n",
        "\\min_{W\\in \\mb{R}^{M\\times k}, H\\in \\mb{R}^{N\\times k}}\\sum_{(u,i), A_{ui}=1}(A_{ui}-W_u H_i^\\top)^2 +\\alpha \\sum_{(u,i), A_{ui}=0}(A_{ui}-W_u H_i^\\top)^2 +\\lambda (\\|W\\|_F^2 + \\|H\\|_F^2)\n",
        "\\end{align}\n",
        "   \n",
        "    - $0<\\alpha< 1$: the weight for 0s is smaller.\n",
        "\n",
        "    - Make the loss more balance (a standard way for class-unbalanced classification)\n",
        "\n",
        "    -  Another motivation from PU (positive-and-unlabeled) learning:\n",
        "        - 0 can be either “negative” or “unobserved” ⇒ lower weight\n",
        "\n",
        "        - 1 is always “positive” ⇒ higher weight\n",
        "\n",
        "- Optimization:\n",
        "\n",
        "  - Assume $M$ users and $N$ items, hidden dimension k.\n",
        "\n",
        "  - Use ALS algorithm: can have time complexity $O(\\|A\\|_0k+Mk^2+Nk^2)$. \n",
        "\n",
        "  - Training time only proportional to number of 1s in $A$.\n"
      ],
      "metadata": {
        "id": "S83x-wp9v_Nf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 2: Matrix factorization with pairwise loss\n",
        "\n",
        "- Idea: using pair-wise comparisons to design the loss function\n",
        "\n",
        "  - If $A_{ui}=1$ and $A_{uj}=0$, then user $u$ prefer $i$ over $j$. \n",
        "\n",
        "  - Therefore $W_uH_i^\\top$ should be larger than $W_uH_j^\\top$.\n",
        "\n",
        "- Matrix factorization with pair-wise loss: $f(W,H)$\n",
        "\\begin{align}\n",
        "f(W,H) = \\sum_{u=1}^M \\left(\\sum_{(i,j): A_{ui}=1, A_{uj}=0}\\ell\\left(W_u(H_i^\\top - H_j^\\top)\\right)\\right)\n",
        "\\end{align}\n",
        "The optimization problem is \n",
        "\\begin{align}\n",
        "\\min_{W\\in \\mb{R}^{M\\times k}, H\\in \\mb{R}^{N\\times k}} f(W,H)\n",
        "\\end{align}\n",
        "\n",
        "- The choice of the loss function: Classification loss on $W_u(H_i^\\top - H_j^\\top)$. We want this to be positive. \n",
        "\n",
        "  - Hinge loss: \n",
        "     \\begin{align}\n",
        "     \\ell\\left(z\\right) = \\max(0, 1-z)\n",
        "     \\end{align}\n",
        "\n",
        "  - Logistic loss (also called Bayesian Personalized Ranking, BPR)\n",
        "    \\begin{align}\n",
        "     \\ell\\left(z\\right) = \\log(1+e^{-z})\n",
        "     \\end{align}\n",
        "    \n",
        "### Algorithm: SGD\n",
        "- Each iteration sample $(u,i,j)$ by\n",
        "   - sample a user $u$\n",
        "\n",
        "   - sample a positive item $i$ ($A_{ui}=1$)\n",
        "\n",
        "   - Sample a negative (unobserved) item $j$ ($A_{uj}=0$)\n",
        "\n",
        "- The sampled loss function: \n",
        " $$\\ell(W_u, H_i, H_j)=\\log(1+\\exp(-W_u(H_i^\\top - H_j^\\top))) $$    \n",
        "\n",
        "\n",
        "- Update $W_u, H_i, H_j$ by SGD:\n",
        "  - $W_u\\leftarrow W_u -\\nabla_{W_u}\\ell(W_u, H_i, H_j)$ \n",
        "\n",
        "  - $H_i\\leftarrow H_i -\\nabla_{H_i}\\ell(W_u, H_i, H_j)$\n",
        "\n",
        "  - $H_j\\leftarrow H_j -\\nabla_{H_j}\\ell(W_u, H_i, H_j)$\n",
        "\n"
      ],
      "metadata": {
        "id": "Uu6vW2uE6B9c"
      }
    }
  ]
}