{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW4Q1-3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN+cowgP04pUaYQKXnUDNFJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yexf308/AdvancedMachineLearning/blob/main/Homework/HW4/HW4Q1_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9Qb1n7Xe9QSM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b97361d5-eea8-4b7c-d8ee-1a79e742fcd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ],
      "source": [
        "%pylab inline \n",
        "import numpy.linalg as LA\n",
        "from time import time\n",
        "import pandas as pd\n",
        "import os \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Machine Learning\n",
        "# Homework 4\n",
        "## Homework guideline\n",
        "\n",
        "- The deadline is May 5th 10:30am. Submission after the deadline will not be graded. \n",
        "\n",
        "- Q1-4 are individual homework. \n",
        "\n",
        "- Submit your work(your reasoning and your code) as a SINGLE .ipynb document. Please rename the document as _HW1_YOURNAME.ipynb_ (for example, _HW1_FELIX.ipynb_). You are responsible for checking that you have correctly submitted the correct document. If your code cannot run, you may receive NO point.\n",
        "\n",
        "- For group homework, only one submission is needed and other group members just need to mention which group belong to.  \n",
        "\n",
        "- Please justify all short answers with a brief explanation.\n",
        "\n",
        "- You only use the Python packages included in the following cell. You are not allowed to use other advanced package or modules unless you are permitted to. \n",
        "\n",
        "- In your final submission include the plots produced by the unedited code as presented below, as well as any additional plots produced after editing the code during the course of a problem. You may find it necessary to copy/paste relevant code into additional cells to accomplish this.\n",
        "\n",
        "- Feel free to use the lecture notes and other resources.\n",
        "\n",
        "- Colab is preferred. However, if you use Anaconda, please download the .mat file locally and save it to the same folder as this homework file. "
      ],
      "metadata": {
        "id": "NE4cZ3A6Ry2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\def\\m#1{\\mathbf{#1}}$\n",
        "$\\def\\mb#1{\\mathbb{#1}}$\n",
        "$\\def\\c#1{\\mathcal{#1}}$\n",
        "\n",
        "# Q1: Theory\n",
        "\n",
        "## Q1.1 LLE (10pt)\n",
        "The local covariance matrix $C$ for the particular data point $\\m{x}$ is defined as \n",
        "$$ C_{jk}=(\\m{x}-\\eta^{(j)})^\\top (\\m{x}-\\eta^{(k)}) $$\n",
        "where $\\{\\eta^{(j)}\\}_{j=1}^K$ is $K$-nearest neighbours of $\\m{x}$. \n",
        "\n",
        "### Q1.1.1 Faster implementation\n",
        "A much faster way to calculate $C$ is \n",
        "\\begin{align}\n",
        "C_{jk} = \\frac{1}{2}(D_j+D_k -D_{jk})\n",
        "\\end{align}\n",
        "where $D_{jk}$ is the square distance between $j$th and $k$th neighbours of $\\m{x}$, $D_j$ is the square distance between $\\m{x}$ and its $j$-th neighbour.\n",
        "It requires significantly less user input than the complete matrix of pairwise distances. **Show** this implementation is exactly the local covariance matrix.  \n"
      ],
      "metadata": {
        "id": "YD6D2CtM9WTa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your solution:"
      ],
      "metadata": {
        "id": "VyDhC6PE9cvJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "### Q1.1.2: (optional) More involved analysis\n",
        "- Please estimate the upper bound and lower bound percentage of the data entries in the matrix $D$ is used. \n",
        "\n",
        "- It is NOT possible to recover manifold structure from even less user inputâ€”say, just the pairwise distances between each data point and its nearest neighbor. Please give a counter example. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lQI88Y1TxsWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your solution:"
      ],
      "metadata": {
        "id": "e9NT-rJk7wLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## Q1.2 t-SNE (15pt)\n",
        "- In SNE, the objective function is \n",
        "$L(\\m{Z}) =\\sum_{i}\\sum_j P_{j|i}\\log\\frac{P_{j|i}}{Q_{j|i}}.$\n",
        "\n",
        " **Show** the gradient for each embedding vector\n",
        "\\begin{align}\n",
        "\\nabla_{\\m{z}^{(i)}} L(\\m{Z})= 2\\sum_j (\\m{z}^{(i)}-\\m{z}^{(j)})(P_{j|i}-Q_{j|i}+P_{i|j}-Q_{i|j})\n",
        "\\end{align}\n",
        "\n",
        "- In symmetric SNE, define \n",
        "$P_{ij} = \\frac{P_{i|j}+P_{j|i}}{2N}, \\ Q_{ij} = \\frac{Q_{i|j}+Q_{j|i}}{2N}.$\n",
        "The objective function is \n",
        "$L(\\m{Z}) = \\sum_{i\\ne j} P_{ij}\\log\\frac{P_{ij}}{Q_{ij}}$.\n",
        "\n",
        " **Show** the gradient is \n",
        "\\begin{align}\n",
        "\\nabla_{\\m{z}^{(i)}} L(\\m{Z}) =4\\sum_j (\\m{z}^{(i)}-\\m{z}^{(j)})(P_{ij}-Q_{ij})\n",
        "\\end{align}\n",
        "\n",
        "- In t-SNE, refine $Q_{ij}$ as $Q_{ij}= \\frac{(1+\\|\\m{z}^{(i)}-\\m{z}^{(j)}\\|^2)^{-1}}{\\sum_{k\\ne l} (1+\\|\\m{z}^{(k)}-\\m{z}^{(l)}\\|^2)^{-1}}$ and leave $P_{ij}$ as is. \n",
        "\n",
        " **Show** the gradient is \n",
        "\\begin{align}\n",
        "\\nabla_{\\m{z}^{(i)}} L(\\m{Z}) =4\\sum_j (\\m{z}^{(i)}-\\m{z}^{(j)})(P_{ij}-Q_{ij})(1+\\|\\m{z}^{(i)}-\\m{z}^{(j)}\\|^2)^{-1}\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "rZWHWbNZxJcX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your solution:"
      ],
      "metadata": {
        "id": "RFRVP2wm7zJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2: t-SNE: early exaggeration (50pt)\n",
        "One difficulty is that the speed with which t-SNE algorithm converges slows down as the number of points $N$ increases. The algorithm requires many more iterations to converge. In the original paper by Van der Maaten and Hinton, they proposes one way to improve the optimization is called \"early exaggeration\", \n",
        "**multiply all of the $p_{ij}$ by $\\alpha$ in the initial stages of the optimization.** In their later paper (2014), \n",
        "\n",
        "   \"During the first 250 learning iterations, we multiplied all $p_{ij}$ values by\n",
        "a user-defined constant $\\alpha  > 1$. [. . .] this trick enables t-SNE to find a\n",
        "better global structure in the early stages of the optimization by creating\n",
        "very tight clusters of points that can easily move around in the embedding\n",
        "space. In preliminary experiments, we found that this trick becomes\n",
        "increasingly important to obtain good embeddings when the data set size increases, as it becomes harder for the optimization to\n",
        "find a good global structure when there are more points in the embedding\n",
        "because there is less space for clusters to move around. In our experiments,\n",
        "we fix $\\alpha$  = 12.\" \n",
        "\n",
        "### Q2.1: Different parameters\n",
        "We will perform the early exaggeration methods on the following  MNIST dataset. \n",
        "\n",
        "- We will first try with the classical parameter: $\\alpha=12$ for the first 250 learning iterations and $\\alpha=1$ for the rest learning iterations. The learning rate $h$ (i.e., step-size in the gradient descent) is 200. \n",
        "\n",
        "- We will then try a canonical setting for the parameters which the algorithm applied to clustered data converges provably at an exponential rate: $\\alpha\\sim N/10$ for the first 250 learning iterations and $\\alpha=1$ for the rest learning iterations. The learning rate $h$ (i.e., step-size in the gradient descent) is 1. \n",
        "\n",
        "For both methods, make sure the initial coordinates $\\mathbf{Z}_0$ are the same. You use like `numpy.random.seed(42)` so you'll always get the same random number sequence. \n",
        "\n",
        "**Make sure to plot these intermediate steps in the early exaggeration phase!**\n",
        "\n",
        "\n",
        "\n",
        "### Q2.2: Connection to spectral methods \n",
        "The transition matrix $\\mathbf{M}$ of the Markov chain is given by\n",
        "\\begin{align}\n",
        "\\mathbf{M}_{ij}=\\begin{cases}1- \\sum_{k\\ne i}P_{ik} & \\text{if } i = j\\\\ P_{ij} & \\text{Otherwise} \\end{cases}\n",
        "\\end{align}\n",
        "We propose to visualize the point set after $k$ iterations as follows: start with the same initial coordinates $\\mathbf{Z}_0$ as before, and calculate $\\mathbf{Z}_k = \\mathbf{M}^k \\mathbf{Z}_0$ by performing the matrix multiplications. Each row of $\\mathbf{Z}_k$ interpretes as coordinates in $\\mathbb{R}^2$. This creates t-SNE-style visualizations for spectral methods. Please run this spectral methods for 250 iterations, and plot the embedding result. \n",
        "**Make sure to plot these intermediate steps!**\n",
        "\n",
        "### Q2.3: Compare\n",
        "- Does t-SNE, in its early exaggeration phase, perform better with the classical parameter choices of $\\alpha = 12,h= 200$ than it does with $\\alpha \\sim N/10, h\\sim 1$? \n",
        "\n",
        "- Does spectral methods look qualitatively equivalent to the behavior of t-SNE with the parameter choice $\\alpha \\sim N/10, h\\sim 1$ in its early exaggeration phase? \n",
        "\n",
        "\n",
        "### Q2.4: Try to embed on 3D (optional)\n",
        "\n",
        "You can try to embed to three-dimensional space. \n"
      ],
      "metadata": {
        "id": "VDipbLwmOhrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "digits = load_digits() \n",
        "\n",
        "X = digits.data\n",
        "y = digits.target \n",
        "\n",
        "X = X[(y==5) | (y==6) | (y==7) | (y==8)|(y==9)]\n",
        "y = y[(y==5) | (y==6) | (y==7) | (y==8) | (y==9)]\n",
        "n = X.shape[0]\n",
        "print(\"\\nThis data set contains \" + str(n) + \" samples\")\n",
        "print(\"\\nDimensions of the  data set: \")\n",
        "print(X.shape)"
      ],
      "metadata": {
        "id": "Bt93wm1IUd2E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d707423c-e5fd-4a40-f1ca-19281290ba78"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "This data set contains 896 samples\n",
            "\n",
            "Dimensions of the  data set: \n",
            "(896, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gCRiFxdZT83w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your solution:"
      ],
      "metadata": {
        "id": "fxCH0wMuTxWC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3: UMAP (25pt)\n",
        "Implement UMAP on the previous dataset and embed the data on 2D and 3D. Please plot your result with the various setting for `n_neigh` and `min_dist` parameters. \n",
        "\n",
        "- Compare the performance with t-SNE. How much more global structure is preserved with UMAP, particularly with larger values of `n_neigh`. \n",
        "\n",
        "\n",
        "- Compare the run-time with t-SNE. How much faster is UMAP algorithm than t-SNE. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GIrAAe5czvjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code starts here"
      ],
      "metadata": {
        "id": "xLFsYSiJ5gZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your solution:"
      ],
      "metadata": {
        "id": "fcse28px5hDS"
      }
    }
  ]
}