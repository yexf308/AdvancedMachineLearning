{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hidden_Markov_Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPE/yCyjq8EEuSJaFROfbfQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yexf308/AdvancedMachineLearning/blob/main/Hidden_Markov_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NY40DdLG0wWk"
      },
      "outputs": [],
      "source": [
        "%pylab inline \n",
        "from IPython.display import Image\n",
        "import numpy.linalg as LA"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\def\\m#1{\\mathbf{#1}}$\n",
        "$\\def\\mb#1{\\mathbb{#1}}$\n",
        "$\\def\\c#1{\\mathcal{#1}}$\n",
        "\n",
        "# Hidden Markov Models\n",
        "\n",
        "Hidden Markov Model is an Unsupervised Machine Learning Algorithm which is part of the Graphical Models. However Hidden Markov Model (HMM) often trained using supervised learning method in case training data is available. \n",
        "\n",
        "## Model\n",
        "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/HMM.png?raw=true\" width=\"500\" />\n",
        "\n",
        "Hidden Markov Models assumes that observed dataset $\\{\\mathbf{x}^{(t)}\\}_{t=1}^N\\subset \\mb{R}^d$ are sampled through a generative process:\n",
        "\n",
        "- Generate the sequence of states $\\{z^{(t)}\\}_{t=1}^N$ by a Markov chain whose transition matrix is $M\\in \\mb{R}^{K\\times K}$ starting the initial distribution as the invariant distribution $\\vec\\pi$, where $K$ is number of hidden states. \n",
        "\n",
        "- **Assumption 1:** We assume the Markov chain has reached the stationary,  \n",
        " $$P(z^{(1)})=\\vec\\pi, \\qquad \\vec\\pi = \\vec\\pi M$$\n",
        "\n",
        "- **Assumption 2:** the state at time $t$, $z^{(t)}$, depends only on the state at the previous time $t-1$, $z^{(t-1)}$, i.e.,\n",
        " $$ P(z^{(t)}|z^{(1:t-1)})=P(z^{(t)}|z^{(t-1)})$$\n",
        "\n",
        "\n",
        "- Given the state $z^{(t)}=c$, generate a point $\\mathbf{x}^{(t)}$ from the associated multivariate Gaussian distribution $\\mathcal{N}(\\mu_c, \\Sigma_c)$ with PDF. It is also called **emission distribution**:\n",
        "$$p(\\m{x}|\\mu_c, \\Sigma_c)= \\frac{1}{\\sqrt{(2\\pi)^d|\\Sigma_c|}}\\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\mu_c)^\\top\\Sigma_c^{-1}(\\mathbf{x}-\\mu_c)\\right)$$\n",
        " The emission distribution can be some discrete value as well. Here we just assume this is a Gaussian. \n",
        " \n",
        "\n",
        " - **Assumption 3**: The observed data at time $t$, $\\m{x}^{(t)}$ depends only on the state at time $t$, $z^{(t)}$, i.e.,\n",
        " $$P(\\m{x}^{(t)}|z^{(1:t)}) =P(\\m{x}^{(t)}|z^{(t)})$$ \n",
        "\n",
        "\n",
        "Now the parameter $\\theta=\\{\\pi_c, \\mu_c, \\Sigma_c,  M_{ij}\\}_{c,i,j=1}^K$"
      ],
      "metadata": {
        "id": "oiF9BBWy1N8g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Center problems in HMM\n",
        "- **Evaluation**: Efficiently compute a wide range of\n",
        "conditional probabilities given $\\m{x}^{(1:N)}$: $P(z^{(t)}=k|\\m{x}^{(1:N)},\\theta), P(z^{(t)}=k, z^{(t+1)}=c|\\m{x}^{(1:N)}, \\theta)$. ⟹ the forward-backward algorithm.\n",
        "\n",
        "- **Inference**: Find the optimal parameter $\\theta^*$ for a given sequence of observations. ⟹ the Baum–Welch algorithm.\n",
        "\n",
        "- **Decoding**: Find the most-likely sequence of\n",
        "hidden states, given a sequence of observations, $\\arg\\min_{z^{(1:N)}}P(z^{(1:N)}|\\mathbf{x}^{(1:N)},\\theta)$. ⟹ the Viterbi algorithm."
      ],
      "metadata": {
        "id": "Ti1_KvVP4shS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Likelihood of HMM\n",
        "\n",
        "### Forward probability\n",
        "\n",
        "The likelihood $\\ell(\\theta)=P(\\mathbf{x}^{(1:N)}|\\theta)$. Similarly,\n",
        "$$P(\\mathbf{x}^{(1:t)}|\\theta)=\\sum_{c=1}^K \\underbrace{P(\\mathbf{x}^{(1:t)}, z^{(t)}=c|\\theta)}_{\\triangleq\\alpha_t(c)} $$\n",
        "\n",
        "However, \n",
        "$$P(\\mathbf{x}^{(1:N)}|\\theta)\\ne \\Pi_{i=1}^N \\sum_{c=1}^K \\left(p(\\mathbf{x}^{(i)}, z^{(i)}=c|\\theta) \\right)$$\n",
        "Because these hidden state are not sampled independently!\n",
        "\n",
        "$\\alpha_t(c)$, is also called **forward probability**. \n",
        "\n",
        "\\begin{align}\n",
        "\\alpha_t(c) &= P(\\mathbf{x}^{(1:t)}, z^{(t)}=c|\\theta) = \\sum_{i=1}^K P(\\mathbf{x}^{(1:t)}, z^{(t)}=c, z^{(t-1)}=i|\\theta) \\\\\n",
        "&=\\sum_{i=1}^K P(\\mathbf{x}^{(t)}| z^{(t)}=c, z^{(t-1)}=i, \\mathbf{x}^{(1:t-1)}, \\theta)\\cdot P(z^{(t)}=c |z^{(t-1)}=i, \\mathbf{x}^{(1:t-1)}, \\theta)\\cdot P(\\mathbf{x}^{(1:t-1)}, z^{(t-1)}=i|\\theta) \\\\\n",
        "&= \\sum_{i=1}^K P(\\mathbf{x}^{(t)} |\\mu_c, \\Sigma_c) M_{ic}\\alpha_{t-1}(i)\n",
        "\\end{align}\n",
        "\n",
        "If we use linear algebra, the recurrsion formular has the following compact form, define $\\vec\\alpha_t = [\\alpha_t(1), \\dots, \\alpha_t(K)]$, $B(\\mathbf{x}^{(t)}) =\\text{diag}([P(\\mathbf{x}^{(t)} |\\mu_1, \\Sigma_1), \\dots, P(\\mathbf{x}^{(t)} |\\mu_K, \\Sigma_K)]) $, then\n",
        "$$\\vec\\alpha_t =\\vec\\alpha_{t-1} M B(\\mathbf{x}^{(t)}) $$\n",
        "For convenience, set $\\vec\\alpha_0=\\vec\\pi$. So the forward probability and likelihood are\n",
        "$$\\vec\\alpha_N= \\vec \\pi M B(\\mathbf{x}^{(1)})M B(\\mathbf{x}^{(2)})\\dots M B(\\mathbf{x}^{(N)})$$\n",
        "\n",
        "$$ \\ell(\\theta)= \\vec\\alpha_N \\mathbb{1}^\\top$$\n",
        "where $\\mathbb{1}$ is a row of ones with length $K$. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Mxh-u6E_3hbk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backward probability \n",
        "Define the **backward probability**, $\\beta_t(c) = P(\\mathbf{x}^{(t+1:N)}|z^{(t)}=c, \\theta)$. Note \n",
        "\\begin{align}\n",
        "P(\\mathbf{x}^{(t+1:N)}|z^{(t)}=c, \\theta) &= \\sum_{i=1}^K P(\\mathbf{x}^{(t+1:N)}|z^{(t+1)}=i, \\theta)P(z^{(t+1)}=i|z^{(t)}=c, \\theta) \\\\\n",
        "&=\\sum_{i=1}^K P(\\mathbf{x}^{(t+1)}|z^{(t+1)}=i, \\theta)P(z^{(t+1)}=i|z^{(t)}=c, \\theta)P(\\mathbf{x}^{(t+2:N)}|z^{(t+1)}=i, \\theta)\n",
        "\\end{align}\n",
        "Then \n",
        "\\begin{align}\n",
        "\\beta_t(c)  = \\sum_{i=1}^K P(\\mathbf{x}^{(t+1)}|\\mu_i, \\Sigma_i)M_{ci} \\beta_{t+1}(i)\n",
        "\\end{align}\n",
        "Define $\\vec\\beta_t =[\\beta_t(1),\\dots, \\beta_t(K)]^T$.\n",
        "\n",
        "For convenience, set $\\vec\\beta_T=[1, \\dots, 1]^T$, the recursion formula has the following compact form.\n",
        "\n",
        "$$\\vec\\beta_{t}=MB(\\mathbf{x}^{(t+1)})\\vec\\beta_{t+1}$$"
      ],
      "metadata": {
        "id": "Yu1Q2poxShAV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Evaluation\n",
        "### Forward-Backward algorithm\n",
        "The first probability is \n",
        "\\begin{align}\n",
        " \\boxed{\\gamma_{k}(i)} = P(z^{(i)}=k|\\mathbf{x}^{(1:N)}, \\theta)&=\\frac{P(z^{(i)}=k, \\mathbf{x}^{(1:N)}|\\theta)}{P(\\mathbf{x}^{(1:N)}|\\theta)}\\\\\n",
        "&=\\frac{P(z^{(i)}=k, \\mathbf{x}^{(1:i)}|\\theta)P(\\mathbf{x}^{(i+1:N)}|z^{(i)}=k, \\theta)}{P(\\mathbf{x}^{(1:N)}|\\theta)} \\\\\n",
        "&= \\frac{\\alpha_i(k)\\beta_i(k)}{P(\\mathbf{x}^{(1:N)}|\\theta)} \\propto \\alpha_i(k)\\beta_i(k)\n",
        "\\end{align}\n",
        "This is because in the hidden markov model, \n",
        "$$ P(\\mathbf{x}^{(i+1:N)}|z^{(i)}=k, \\mathbf{x}^{(1:i)})= P(\\mathbf{x}^{(i+1:N)}|z^{(i)}=k).$$\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The second probability is \n",
        "\\begin{align}\n",
        "\\boxed{\\xi_{kc}(i)}&= P(z^{(i)}=k, z^{(i+1)}=c|\\mathbf{x}^{(1:N)} ,\\theta) \\\\\n",
        "&=\\frac{P(z^{(i)}=k, z^{(i+1)}=c,\\mathbf{x}^{(1:N)}|\\theta )}{P(\\mathbf{x}^{(1:N)}|\\theta )} \\\\\n",
        "& = \\frac{P(z^{(i)}=k, \\mathbf{x}^{(1:i)}|\\theta)P(\\mathbf{x}^{(i+2:N)}|z^{(i+1)}=c, \\theta)P(z^{(i+1)}=c|z^{(i)}=k)P(\\mathbf{x}^{(i+1)}|z^{(i+1)}=c)}{P(\\mathbf{x}^{(1:N)}|\\theta )} \\\\\n",
        "&=\\frac{\\alpha_i(k)\\beta_{i+1}(c)M_{kc}P(\\mathbf{x}^{(i+1)}|\\mu_c, \\Sigma_c)}{P(\\mathbf{x}^{(1:N)}|\\theta)} \\propto \\alpha_i(k)\\beta_{i+1}(c)M_{kc}P(\\mathbf{x}^{(i+1)}|\\mu_c, \\Sigma_c)\n",
        "\\end{align}\n",
        "This is because in the hidden markov model,  \n",
        "\\begin{align}\n",
        "P(z^{(i)}, z^{(i+1)},\\mathbf{x}^{(1:N)} ) &=P(z^{(i)}, \\mathbf{x}^{(1:i)})P(\\m{x}^{(i+1:N)}, z^{(i+1)}|z^{(i)}) \\\\\n",
        "& =P(z^{(i)}, \\mathbf{x}^{(1:i)}) P(z^{(i+1)}|z^{(i)})P(\\m{x}^{(i+1:N)}|z^{(i+1)})  \\\\\n",
        "&=P(z^{(i)}, \\mathbf{x}^{(1:i)}) P(z^{(i+1)}|z^{(i)})P(\\m{x}^{(i+1)}|z^{(i+1)})P(\\m{x}^{(i+2:N)}|z^{(i+1)}) \n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "0jGanTbCo2sE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log-sum-exp trick\n",
        "In practice, naively implementing forward algorithm will never work due to arithmetic underflow or overflow. \n",
        "\n",
        "Define $g_t(c)=\\log \\alpha_t(c)$, then \n",
        "\\begin{align}\n",
        " g_t(c) = \\log \\alpha_t(c) = \\log \\sum_{i=1}^K \\exp\\left(l(z^{(t)}=c|z^{(t-1)}=i) + l(\\mathbf{x}^{(t)}|z^{(t)}=c)+g_{t-1}(i)\\right)\n",
        "\\end{align}\n",
        "denoting $l=\\log P$\n",
        "\n",
        "To calculate this, one has to use **log-sum-exp** trick:\n",
        "\n",
        "let’s suppose we would like to compute $\\log\\sum_{i=1}^m\\exp(a_i)$, \n",
        "\n",
        "- choose $b=\\max_{i}a_i$.\n",
        "\n",
        "- \n",
        "\\begin{align}\n",
        "\\log\\sum_{i=1}^m\\exp(a_i) =\\log\\sum_{i=1}^m\\exp(a_i-b)\\exp(b)  = b+\\log\\sum_{i=1}^m\\exp(a_i-b)\n",
        "\\end{align}\n"
      ],
      "metadata": {
        "id": "ClC7-2dKPTQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Expectation-Maximization(EM) algorithm: the general version\n",
        "Please refer to [GMM note](https://github.com/yexf308/MAT592/blob/main/Module2/GMM2.ipynb) for special case of EM. \n",
        "### The general version\n",
        "The goal of EM is to find a maximum likelihood estimate (MLE) estimate in models involving hidden/latent/unobserved variables/data.\n",
        "\n",
        "- Observed data $\\mathbf{x}^{(1:N)}$.\n",
        "\n",
        "- Model $(X, Z)\\sim P(\\mathbf{x}^{(1:N)} ,z^{(1:N)} |\\theta)$. Here $z^{(1:N)}$ represents some collection of unobserved variables. Note EM works\n",
        "best when $P(\\mathbf{x}^{(1:N)} ,z^{(1:N)} |\\theta)$ is an exponential family. \n",
        "\n",
        "- **Goal:** Find $\\theta^*=\\arg\\max_{\\theta} P(\\mathbf{x}^{(1:N)}|\\theta)$. Note the likelihood $P(\\mathbf{x}^{(1:N)}|\\theta) = \\sum_{z^{(1:N)}}P(\\mathbf{x}^{(1:N)} ,z^{(1:N)} |\\theta)$. \n",
        "\n",
        "- **EM Algorithm:**\n",
        " - Initialize $\\theta^1$. \n",
        "\n",
        " - For $t=1, 2, \\dots$ until some convergence criterion is met,\n",
        "\n",
        "    - E-step: Compute the  auxiliary function\n",
        "\n",
        "    \\begin{align} \\boxed{Q(\\color{blue}\\theta, \\color{red}{\\theta^t})=\\mathbb{E}_{Z|X, \\color{red}{\\theta^t}}\\left[\\log P(X, Z|X=\\mathbf{x}^{(1:N)} ,\\color{blue}\\theta)\\right]   = \\sum_{z^{(1:N)}} P(z^{(1:N)} |\\mathbf{x}^{(1:N)} ,\\color{red}{\\theta^t})\\log P(\\mathbf{x}^{(1:N)} ,z^{(1:N)} |\\color{blue} \\theta)}\n",
        "    \\end{align}\n",
        "\n",
        "    - M step: Solve for $\\color{red}{\\theta_{t+1}} = \\arg\\max_\\theta Q(\\color{blue}\\theta, \\color{red}{\\theta^t})$\n",
        "\n",
        "- In practice, we will be able to represent $Q(\\color{blue}\\theta, \\color{red}{\\theta^t})$  analytically as a function $\\theta$. Moreover, we will\n",
        "often be able to analytically maximize $Q(\\color{blue}\\theta, \\color{red}{\\theta^t})$. \n",
        "\n",
        "- It is usually a good idea to introduce some randomization into the initialization $\\theta^1$. \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yIAKZ0p8Qyap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Proof of correctness\n",
        "\n",
        "Expectation-maximization works to improve $Q(\\color{blue}\\theta, \\color{red}{\\theta^t})$ rather than directly improving the log likelihood $\\log P(\\mathbf{x}^{(1:N)}|\\color{blue}\\theta)$. Here it is shown that improvements to the former imply improvements to the latter.\n",
        "\n",
        "By Baye's rule, for any $z^{(1:N)}$ with non-zero probability  $P(z^{(1:N)}|\\mathbf{x}^{(1:N)}, \\color{blue}\\theta)$, \n",
        "\n",
        "$$\\log P(\\mathbf{x}^{(1:N)}|\\color{blue}\\theta) = \\log P(\\mathbf{x}^{(1:N)}, z^{(1:N)}|\\color{blue}\\theta) - \\log P(z^{(1:N)}|\\mathbf{x}^{(1:N)}, \\color{blue}\\theta) $$\n",
        "\n",
        "By multiplying both sides $P(z^{(1:N)}|\\mathbf{x}^{(1:N)}, \\color{red}{\\theta^t})$ and summing over $z^{(1:N)}$, (which is equvilently, take the expectation over possible values of the hidden variable $Z$ under the current parameter estimate $\\color{red}{\\theta^t}$)\n",
        "\\begin{align}\n",
        "\\log P(\\mathbf{x}^{(1:N)}|\\color{blue}\\theta)&=\\sum_{z^{(1:N)}}P(z^{(1:N)}|\\mathbf{x}^{(1:N)}, \\color{red}{\\theta^t})\\log P(\\mathbf{x}^{(1:N)}, z^{(1:N)}|\\color{blue}\\theta) -\\sum_{z^{(1:N)}} P(z^{(1:N)}|\\mathbf{x}^{(1:N)}, \\color{red}{\\theta^t})\\log P(z^{(1:N)}|\\mathbf{x}^{(1:N)}, \\color{blue}\\theta) \\\\\n",
        "& =Q(\\color{blue}\\theta, \\color{red}{\\theta^t}) +H(\\color{blue}\\theta|\\color{red}{\\theta^t})\n",
        "\\end{align}\n",
        "\n",
        "when $\\theta= \\theta^t$, \n",
        "$$\\log P(\\mathbf{x}^{(1:N)}|\\color{red}{\\theta^t})=Q(\\color{red}{\\theta^{t}}, \\color{red}{\\theta^t}) +H(\\color{red}{\\theta^t}|\\color{red}{\\theta^t}) $$\n",
        "\n",
        " Subtracting this last equation from the previous equation gives\n",
        "\\begin{align}\n",
        " \\log P(\\mathbf{x}^{(1:N)}|\\color{blue}\\theta) -\\log P(\\mathbf{x}^{(1:N)}|\\color{red}{\\theta^t}) &=  Q(\\color{blue}\\theta, \\color{red}{\\theta^t}) - Q(\\color{red}{\\theta^{t}}, \\color{red}{\\theta^t}) +H(\\color{blue}\\theta|\\color{red}{\\theta^t})-H(\\color{red}{\\theta^t}|\\color{red}{\\theta^t})   \n",
        "  \\end{align}\n",
        "  \\begin{align}\n",
        " \\boxed{\\log P(\\mathbf{x}^{(1:N)}|\\color{blue}\\theta) -\\log P(\\mathbf{x}^{(1:N)}|\\color{red}{\\theta^t}) \\ge Q(\\color{blue}\\theta, \\color{red}{\\theta^t}) - Q(\\color{red}{\\theta^{t}}, \\color{red}{\\theta^t})}\n",
        " \\end{align}\n",
        "\n",
        " It is due to the Gibbs' inequality that $H(\\color{blue}\\theta|\\color{red}{\\theta^t})\\ge H(\\color{red}{\\theta^t}|\\color{red}{\\theta^t})$.\n",
        "\n",
        " So choose $\\theta$ to maximize $Q(\\color{blue}\\theta, \\color{red}{\\theta^t})$ causes $\\log P(\\mathbf{x}^{(1:N)}|\\color{blue}\\theta)$ improves at least that much. "
      ],
      "metadata": {
        "id": "yKr2TMzYQ4zf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: EM for HMM: Baum–Welch algorithm\n",
        "\n",
        "### E-step\n",
        "The joint probability $P(\\mathbf{x}^{(1:N)} ,z^{(1:N)} |\\theta)$ is \n",
        "\\begin{align}\n",
        "P(\\mathbf{x}^{(1:N)} ,z^{(1:N)} |\\theta)&=P(z^{(1)})\\cdot\\Pi_{i=1}^{N-1}P(z^{(i+1)}|z^{(i)},\\theta) \\cdot\\Pi_{j=1}^N P(\\mathbf{x}^{(j)}|z^{(j)},\\theta) \\\\\n",
        "&=\\pi_{z^{(1)}}\\cdot\\Pi_{i=1}^{N-1} M_{z^{(i)}z^{(i+1)}} \\cdot \\Pi_{j=1}^N P(\\mathbf{x}^{(j)}|\\mu_{z^{(j)}},\\Sigma_{z^{(j)}})\n",
        "\\end{align}\n",
        "Then \n",
        "\\begin{align}\n",
        "\\log P(\\mathbf{x}^{(1:N)} ,z^{(1:N)} |\\theta) = \\log \\pi_{z^{(1)}} + \\sum_{i=1}^{N-1} \\log M_{z^{(i)}z^{(i+1)}} + \\sum_{j=1}^N \\log P(\\mathbf{x}^{(j)}|\\mu_{z^{(j)}},\\Sigma_{z^{(j)}})\n",
        "\\end{align}\n",
        "\n",
        "The  auxiliary function $Q(\\color{blue}\\theta, \\color{red}{\\theta^t})$ is,\n",
        "\n",
        "\\begin{align}\n",
        "Q(\\color{blue}\\theta, \\color{red}{\\theta^t}) &= \\sum_{z^{(1:N)}} P(z^{(1:N)} |\\mathbf{x}^{(1:N)} ,\\color{red}{\\theta^t}) \\log P(\\mathbf{x}^{(1:N)} ,z^{(1:N)} |\\color{blue}\\theta)  \\\\\n",
        "&= \\sum_{z^{(1)}}\\boxed{P(z^{(1)}|\\mathbf{x}^{(1:N)} ,\\color{red}{\\theta^t})}\\log \\pi_{z^{(1)}}+\\sum_{i=1}^{N-1}\\sum_{z^{(i)}z^{(i+1)}} \\boxed{P(z^{(i:i+1)}|\\mathbf{x}^{(1:N)} ,\\color{red}{\\theta^t}) }\\log M_{z^{(i)}z^{(i+1)}} + \\sum_{j=1}^N \\sum_{z^{(j)}}\\boxed{P(z^{(j)}|\\mathbf{x}^{(1:N)} ,\\color{red}{\\theta^t})}\\log P(\\mathbf{x}^{(j)}|\\mu_{z^{(j)}},\\Sigma_{z^{(j)}}) \\\\\n",
        "& =  \\sum_{k=1}^K\\color{red}{\\gamma_{k}^t(1)}\\log \\color{blue}\\pi_{k}+\\sum_{i=1}^{N-1}\\sum_{k,c=1}^K \\color{red}{\\xi_{kc}^t(i)}\\log\\color{blue}{M_{kc}} + \\sum_{j=1}^N \\sum_{k=1}^K \\color{red}{\\gamma_{k}^t(j)}\\log P(\\mathbf{x}^{(j)}|\\color{blue} {\\mu_{k},\\Sigma_{k}})\n",
        "\\end{align}\n",
        "\n",
        "where $\\color{red}{\\gamma_{k}^t(i)} = P(z^{(i)}=k|\\mathbf{x}^{(1:N)} ,\\color{red}{\\theta^t})$ and $\\color{red}{\\xi_{kc}^t(i)}= P(z^{(i)}=k, z^{(i+1)}=c|\\mathbf{x}^{(1:N)} ,\\color{red}{\\theta^t}) $. How to find $\\gamma_{k}(i)$ and $\\xi_{kc}(i)$?\n",
        "\n",
        "\n",
        "\n",
        "### M-step \n",
        " For the M-step, we need to find a value of $\\theta$ maximizing $Q(\\theta, \\theta^t)$.  Fortunately, it turns out that we can often do this analytically. \n",
        "\n",
        " - First, to maximize with respect to $\\mu_k$ and $\\Sigma_k$, i.e., \n",
        " $$ 0= \\nabla_{\\mu_k}Q(\\color{blue}\\theta, \\color{red}{\\theta^t}) = \\sum_{j=1}^N \\color{red}{\\gamma_{k}^t(j)} \\nabla_{\\mu_k} \\log P(\\mathbf{x}^{(j)}|\\color{blue}{\\mu_{k},\\Sigma_{k}}) $$\n",
        "\n",
        "  $$ 0= \\nabla_{\\Sigma_k}Q(\\color{blue}\\theta, \\color{red}{\\theta^t}) = \\sum_{j=1}^N \\color{red}{\\gamma_{k}^t(j)} \\nabla_{\\Sigma_k} \\log P(\\mathbf{x}^{(j)}|\\color{blue}{\\mu_{k},\\Sigma_{k}}) $$\n",
        "\n",
        "  Then we have the analytical solution which is the solution of weighted MLE\n",
        "  $$\\mu_k^{t+1} = \\frac{\\sum_{j=1}^N \\color{red}{\\gamma_{k}^t(j)} \\mathbf{x}^{(j)}}{\\sum_{j=1}^N \\color{red}{\\gamma_{k}^t(j)}} $$\n",
        "\n",
        "$$\\Sigma_k^{t+1} =\\frac{\\sum_{j=1}^N \\color{red}{\\gamma_{k}^t(j)} \\left[\\mathbf{x}^{(i)}-\\mu_k^{t+1}\\right]\\left[\\mathbf{x}^{(i)}-\\mu_k^{t+1}\\right]^T }{\\sum_{j=1}^N \\color{red}{\\gamma_{k}^t(j)}} $$\n",
        "\n",
        "- Second, to maximize with respect to $\\vec\\pi$. The constraint is $\\sum_{k}\\pi_k = 1$. We can do this analytically using the method of Lagrange multipliers, the result is \n",
        " \n",
        " $$ \\pi_k^{t+1} =\\frac{\\gamma_k^t(1)}{\\sum_{c=1}^K \\gamma_c^t(1)} $$\n",
        "\n",
        "\n",
        "- Third, to maximize with respect to the Markov transition matrix $M$. The contraint is $\\sum_{c=1}^K M_{kc} =1$. We can do this analytically using\n",
        "Lagrange multipliers as well. \n",
        "\n",
        "$$M_{kc}^{t+1}= \\frac{\\sum_{j=2}^N \\xi_{kc}^t(j)}{\\sum_{j=2}^N \\sum_{c=1}^K \\xi_{kc}^t(j)}. $$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RqwFaRcbSCYE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "If we look at the MLE, $\\gamma_k(i)$ shows up in both denorminator and numerator. So what matters is the relative ratio $\\gamma_k(i)$ for different $i$. \n",
        "\n",
        "So we can renormalized $\\vec\\alpha_i$ and $\\vec\\beta_i$ at each $i$. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "illT-_UNSjV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Task 3: Decoding - Viterbi algorithm\n",
        "Some backgound information. \n",
        "- If $c\\ge 0, f(x)\\ge 0$, then $\\max_x cf(x)=c\\max_xf(x)$. \n",
        "\n",
        "- $\\max_{xy} f(x,y)=\\max_x\\max_y f(x,y)$. \n",
        "\n",
        "The goal of the Viterbi algorithm is to find a\n",
        "\\begin{align}\n",
        "z_{1:N}^* = \\arg\\max_{z_{1:N}} P(z_{1:N}|\\m{x}_{1:N})\n",
        "\\end{align}\n",
        "Note $P(\\m{x}_{1:N})$ is constant with respect to $z_{1:N}$, this  is equivalent to \n",
        "\n",
        "\\begin{align}\n",
        "z_{1:N}^* = \\arg\\max_{z_{1:N}} P(z_{1:N},\\m{x}_{1:N})\n",
        "\\end{align}\n",
        "\n",
        "Define $\\mu_1(z_1)=p(z_1)p(\\m{x}_1|z_1)$. Writing\n",
        "out the factorization implied by the graphical model for an HMM,\n",
        "\\begin{align}\n",
        "P(z_{1:N},\\m{x}_{1:N}) = \\underbrace{P(z_1)P(\\m{x}_1|z_1)}_{=\\mu_1(z_1)} P(z_2|z_1) P(\\m{x}_2|z_2) \\Pi_{t=3}^N P(z_{t}|z_{t-1}) P(\\m{x}_t|z_t)\n",
        "\\end{align}\n",
        "We have\n",
        "\\begin{align}\n",
        "\\max_{z_{1:N}} P(z_{1:N},\\m{x}_{1:N}) &= \\max_{z_{2:N}}\\left(\\underbrace{\\max_{z_1} \\mu_1(z_1)  P(z_2|z_1) P(\\m{x}_2|z_2) }_{=\\mu_2(z_2)}\\right)\\Pi_{t=3}^N P(z_{t}|z_{t-1}) P(\\m{x}_t|z_t) \\\\\n",
        "&=\\max_{z_{3:N}}\\left(\\underbrace{\\max_{z_2} \\mu_2(z_2)  P(z_3|z_2) P(\\m{x}_3|z_3) }_{=\\mu_3(z_3)}\\right)\\Pi_{t=4}^N P(z_{t}|z_{t-1}) P(\\m{x}_t|z_t) \\\\\n",
        "&=\\max_{z_{j:N}}\\left(\\underbrace{\\max_{z_{j-1}} \\mu_{j-1}(z_{j-1})  P(z_j|z_{j-1}) P(\\m{x}_{j}|z_j) }_{=\\mu_j(z_j)}\\right)\\Pi_{t=j+1}^N P(z_{t}|z_{t-1}) P(\\m{x}_t|z_t) = \\max_{z_N}\\mu_N(z_N)\n",
        "\\end{align}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iQjcJYzcxnwj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Algorithm for  $\\arg\\max_{z_{1:N}} P(z_{1:N},\\m{x}_{1:N})$ \n",
        "we can compute $\\arg\\max_{z_{1:N}} P(z_{1:N},\\m{x}_{1:N})$ via the following algorithm:\n",
        "\n",
        "- For each $z_1= 1,\\dots, K$, compute $\\mu_1(z_1)= P(z_1)P(\\m{x}_1|z_1) \\qquad O(K)$\n",
        "\n",
        "  and record the optimal $z_1^*=\\arg\\max_{z_1}P(z_1)P(\\m{x}_1|z_1)$. \n",
        "\n",
        "- For each $j=2, \\dots, N$, for each $z_j=1,\\dots, K$, compute\n",
        "\\begin{align}\n",
        "\\mu_j(z_j) =\\max_{z_{j-1}} \\mu_{j-1}(z_{j-1})  P(z_j|z_{j-1}) P(\\m{x}_{j}|z_j)  \\qquad O(NK^2)\n",
        "\\end{align}\n",
        "and record the optimal $z_{j-1}^*$ as \n",
        "\\begin{align}\n",
        "z_{j-1}^* =\\arg\\max_{z_{j-1}} \\mu_{j-1}(z_{j-1})  P(z_j|z_{j-1}) P(\\m{x}_{j}|z_j)\n",
        "\\end{align}\n",
        "\n",
        "- Compute $ \\max_{z_N}\\mu_N(z_N)$ and $z_N^* = \\arg\\max_{z_N} \\mu_N(z_N)$. $\\qquad O(K)$\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "We sequentially obtained the optimal sequence $z_{1:N}^*$, so need to verify it actually attains the maximum. Note for each $j=N, N-1,\\dots, 2$,\n",
        "$$ \\mu_j(z_j^*) =\\max_{z_{j-1}} \\mu_{j-1}(z_{j-1})  P(z_j^*|z_{j-1}) P(\\m{x}_{j}|z_j^*) $$\n",
        "We can plug in the expression repeatly on $\\mu_N(z_N^*)$, \n",
        "\\begin{align}\n",
        "\\mu_N(z_N^*) = P(z^*_{1:N},\\m{x}_{1:N} )=\\max_{z^{1:N}}P(z_{1:N},\\m{x}_{1:N} )\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "---\n",
        "###  Fixing arithmetic underflow/overflow\n",
        "Note $\\mu_j(z_j^*)$ is usually very small number since it multiply the probability more than hundreds times so the value may\n",
        "have \"arithmetic underflow\", i.e., the value is too small and it is considered as zero. \n",
        "\n",
        "The key is to log probability!\n",
        "\n",
        "Define $\\ell= \\log P$, i.e., $\\ell(z_1)=\\log P(z_1), \\ell(z_t|z_{t-1})=\\log P(z_t|z_{t-1})$ and $\\ell(\\m{x}_t|z_t)=\\log P(\\m{x}_t|z_t)$.\n",
        "\n",
        "The algorithm described above works if we use $f_i(z_j)$ in place of $\\log\\mu_j(z_j)$,\n",
        "\\begin{align}\n",
        "&f_1(z_1) = \\ell(z_1) +\\ell(\\m{x}_1|z_1) \\\\\n",
        "&f_j(z_j) = \\max_{z_{j-1}}\\left( f_{j-1}(z_{j-1}) +\\ell(z_j|z_{j-1}) +\\ell(\\m{x}_j|z_j) \\right)\n",
        "\\end{align}\n",
        "\n",
        "The reason why it works is because log is order-preserving.. \n",
        "Consequently, choosing $z_j^*$ in this way is equivalent to the earlier way. \n"
      ],
      "metadata": {
        "id": "ovqHh4EDNovr"
      }
    }
  ]
}